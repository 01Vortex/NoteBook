# 分库分表完全指南

> 基于 Java 8 + Spring Boot 2.7.18 + ShardingSphere 的分库分表最佳实践
> 本笔记涵盖分库分表原理、方案选择、实战配置、数据迁移及常见问题解决

---

## 目录

1. [分库分表概述](#1-分库分表概述)
2. [分库分表策略](#2-分库分表策略)
3. [ShardingSphere-JDBC 实战](#3-shardingsphere-jdbc-实战)
4. [分片算法详解](#4-分片算法详解)
5. [读写分离](#5-读写分离)
6. [分布式事务](#6-分布式事务)
7. [分布式主键](#7-分布式主键)
8. [数据迁移](#8-数据迁移)
9. [运维与监控](#9-运维与监控)
10. [常见错误与解决方案](#10-常见错误与解决方案)

---

## 1. 分库分表概述

### 1.1 什么是分库分表？

当单表数据量达到千万级别，或者单库并发达到瓶颈时，就需要考虑分库分表了。

**通俗理解**：想象一个图书馆，所有书都放在一个书架上，找书会很慢。分库分表就像把书按类别分到不同的书架（分表），甚至分到不同的楼层（分库），这样找书就快多了。

```
┌─────────────────────────────────────────────────────────────────────┐
│                    分库分表示意图                                    │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  单库单表（数据量大时性能差）                                        │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │                    Database                                  │   │
│  │  ┌─────────────────────────────────────────────────────┐    │   │
│  │  │              user 表 (1亿条数据)                      │    │   │
│  │  └─────────────────────────────────────────────────────┘    │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                              ↓                                      │
│                         分库分表后                                   │
│  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐  │
│  │    Database_0    │  │    Database_1    │  │    Database_2    │  │
│  │  ┌────────────┐  │  │  ┌────────────┐  │  │  ┌────────────┐  │  │
│  │  │  user_0    │  │  │  │  user_0    │  │  │  │  user_0    │  │  │
│  │  │  user_1    │  │  │  │  user_1    │  │  │  │  user_1    │  │  │
│  │  │  user_2    │  │  │  │  user_2    │  │  │  │  user_2    │  │  │
│  │  └────────────┘  │  │  └────────────┘  │  │  └────────────┘  │  │
│  └──────────────────┘  └──────────────────┘  └──────────────────┘  │
│                                                                      │
│  每个库 3 张表，共 3 个库 = 9 张表                                   │
│  1亿数据分散到 9 张表，每张表约 1100 万条                            │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.2 为什么需要分库分表？

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分库分表的必要性                                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 单表数据量过大的问题:                                                │
│ • 查询性能下降（即使有索引，B+树层级增加）                           │
│ • 写入性能下降（索引维护成本增加）                                   │
│ • DDL 操作困难（加字段、加索引耗时长）                               │
│ • 备份恢复困难（单表备份文件过大）                                   │
│                                                                      │
│ 单库并发过高的问题:                                                  │
│ • 数据库连接数有限（MySQL 默认 151）                                 │
│ • CPU、内存、磁盘 IO 成为瓶颈                                        │
│ • 主从复制延迟增加                                                   │
│                                                                      │
│ 什么时候需要分库分表？                                               │
│ • 单表数据量超过 500 万 ~ 1000 万（经验值）                          │
│ • 单库 QPS 超过 5000（取决于硬件配置）                               │
│ • 单库数据量超过 100GB                                               │
│ • 业务增长趋势明显，预计很快达到瓶颈                                 │
│                                                                      │
│ ⚠️ 注意：分库分表会带来复杂性，不要过早优化！                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.3 分库分表的方式

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分库分表方式                                                         │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 1. 垂直拆分                                                          │
│    ┌─────────────────┐      ┌─────────────┐  ┌─────────────┐        │
│    │   user 表       │      │  user_base  │  │ user_detail │        │
│    │ id              │  →   │ id          │  │ id          │        │
│    │ name            │      │ name        │  │ address     │        │
│    │ age             │      │ age         │  │ hobby       │        │
│    │ address         │      └─────────────┘  │ intro       │        │
│    │ hobby           │                       └─────────────┘        │
│    │ intro           │      按字段拆分，常用字段和不常用字段分开     │
│    └─────────────────┘                                               │
│                                                                      │
│ 2. 水平拆分                                                          │
│    ┌─────────────────┐      ┌─────────────┐  ┌─────────────┐        │
│    │   user 表       │      │   user_0    │  │   user_1    │        │
│    │ 1亿条数据       │  →   │ 5000万条    │  │ 5000万条    │        │
│    └─────────────────┘      └─────────────┘  └─────────────┘        │
│                             按行拆分，数据分散到多张表               │
│                                                                      │
│ 3. 垂直分库                                                          │
│    ┌─────────────────┐      ┌─────────────┐  ┌─────────────┐        │
│    │   单一数据库    │      │  用户库     │  │  订单库     │        │
│    │ user 表         │  →   │ user 表     │  │ order 表    │        │
│    │ order 表        │      └─────────────┘  └─────────────┘        │
│    │ product 表      │      ┌─────────────┐                         │
│    └─────────────────┘      │  商品库     │                         │
│                             │ product 表  │                         │
│                             └─────────────┘                         │
│                             按业务拆分到不同数据库                   │
│                                                                      │
│ 4. 水平分库                                                          │
│    ┌─────────────────┐      ┌─────────────┐  ┌─────────────┐        │
│    │   单一数据库    │      │   db_0      │  │   db_1      │        │
│    │ user 表(1亿)    │  →   │ user(5000万)│  │ user(5000万)│        │
│    └─────────────────┘      └─────────────┘  └─────────────┘        │
│                             同一张表的数据分散到多个数据库           │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.4 分库分表带来的问题

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分库分表带来的挑战                                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 1. 跨库 JOIN                                                         │
│    • 问题：不同库的表无法直接 JOIN                                   │
│    • 解决：应用层组装、冗余数据、全局表                              │
│                                                                      │
│ 2. 跨库事务                                                          │
│    • 问题：分布式事务复杂，性能差                                    │
│    • 解决：最终一致性、TCC、Seata                                    │
│                                                                      │
│ 3. 分布式主键                                                        │
│    • 问题：自增 ID 会冲突                                            │
│    • 解决：雪花算法、UUID、号段模式                                  │
│                                                                      │
│ 4. 跨库分页排序                                                      │
│    • 问题：需要从多个库查询后合并                                    │
│    • 解决：流式归并、限制深度分页                                    │
│                                                                      │
│ 5. 数据迁移                                                          │
│    • 问题：历史数据迁移复杂                                          │
│    • 解决：双写、增量同步                                            │
│                                                                      │
│ 6. 扩容困难                                                          │
│    • 问题：增加分片需要数据重新分布                                  │
│    • 解决：一致性哈希、预分片                                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 1.5 主流分库分表方案

```
┌─────────────────────────────────────────────────────────────────────┐
│ 主流分库分表中间件对比                                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 方案              │ 类型      │ 优点              │ 缺点            │
│───────────────────┼───────────┼───────────────────┼─────────────────│
│ ShardingSphere    │ 客户端    │ 功能全面、社区活跃│ 配置复杂        │
│ (推荐)            │ /代理     │ Apache 顶级项目   │                 │
│───────────────────┼───────────┼───────────────────┼─────────────────│
│ MyCat             │ 代理      │ 成熟稳定          │ 维护不活跃      │
│───────────────────┼───────────┼───────────────────┼─────────────────│
│ TDDL              │ 客户端    │ 阿里内部大规模    │ 开源版功能少    │
│                   │           │ 使用验证          │                 │
│───────────────────┼───────────┼───────────────────┼─────────────────│
│ Vitess            │ 代理      │ YouTube 出品      │ 学习成本高      │
│                   │           │ 云原生            │                 │
│───────────────────┼───────────┼───────────────────┼─────────────────│
│ TiDB              │ NewSQL    │ 兼容 MySQL        │ 运维成本高      │
│                   │           │ 自动分片          │                 │
│                                                                      │
│ 本笔记主要介绍 ShardingSphere-JDBC，它是目前最主流的方案             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 2. 分库分表策略

### 2.1 分片键选择

分片键（Sharding Key）是决定数据分布的关键字段，选择不当会导致数据倾斜。

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分片键选择原则                                                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ ✅ 好的分片键:                                                       │
│ • 查询条件中经常出现（避免全库扫描）                                 │
│ • 数据分布均匀（避免热点）                                           │
│ • 不会频繁修改（修改分片键需要迁移数据）                             │
│ • 业务关联性强（相关数据在同一分片）                                 │
│                                                                      │
│ 常见分片键选择:                                                      │
│ • 用户表: user_id                                                    │
│ • 订单表: user_id（同一用户的订单在同一分片，便于查询）              │
│ • 日志表: 时间字段（按时间范围分片）                                 │
│                                                                      │
│ ❌ 不好的分片键:                                                     │
│ • 状态字段（值域小，分布不均）                                       │
│ • 自增 ID（新数据集中在最后一个分片）                                │
│ • 频繁更新的字段                                                     │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 2.2 常见分片算法

```java
/**
 * 1. 取模分片（最常用）
 * 适用场景：数据量可预估，分片数固定
 */
// 分片数 = 4
// user_id = 12345
// 分片位置 = 12345 % 4 = 1 → user_1

/**
 * 2. 范围分片
 * 适用场景：按时间或 ID 范围查询
 */
// ID 1-1000000 → user_0
// ID 1000001-2000000 → user_1
// ID 2000001-3000000 → user_2

/**
 * 3. 哈希分片
 * 适用场景：分片键不是数字类型
 */
// hash(user_name) % 4 = 分片位置

/**
 * 4. 一致性哈希
 * 适用场景：需要动态扩容
 */
// 将分片节点映射到哈希环上
// 数据根据哈希值顺时针找到最近的节点

/**
 * 5. 复合分片
 * 适用场景：多维度查询
 */
// 先按 user_id 分库，再按 order_id 分表
```

### 2.3 分片数量规划

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分片数量规划建议                                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 单表数据量建议:                                                      │
│ • MySQL InnoDB: 500万 ~ 2000万 条/表                                 │
│ • 超过 2000 万性能明显下降                                           │
│                                                                      │
│ 分片数计算:                                                          │
│ • 预估总数据量 / 单表建议数据量 = 分片数                             │
│ • 例: 1亿数据 / 1000万 = 10 个分片                                   │
│                                                                      │
│ 分片数建议:                                                          │
│ • 选择 2 的幂次方（2, 4, 8, 16, 32...）便于扩容                      │
│ • 预留 2-3 倍扩展空间                                                │
│ • 分库数 × 分表数 = 总分片数                                         │
│                                                                      │
│ 示例规划:                                                            │
│ • 当前数据量: 5000万                                                 │
│ • 预计 3 年后: 5亿                                                   │
│ • 单表目标: 500万                                                    │
│ • 需要分片数: 5亿 / 500万 = 100                                      │
│ • 实际规划: 4库 × 32表 = 128 分片（预留空间）                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 3. ShardingSphere-JDBC 实战

ShardingSphere-JDBC 是 Apache ShardingSphere 的 JDBC 驱动版本，以 jar 包形式嵌入应用，无需额外部署。

### 3.1 环境准备

```xml
<!-- pom.xml -->
<dependencies>
    <!-- Spring Boot -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    
    <!-- ShardingSphere JDBC -->
    <dependency>
        <groupId>org.apache.shardingsphere</groupId>
        <artifactId>shardingsphere-jdbc-core-spring-boot-starter</artifactId>
        <version>5.3.2</version>
    </dependency>
    
    <!-- MySQL 驱动 -->
    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <version>8.0.33</version>
    </dependency>
    
    <!-- MyBatis-Plus（可选，也可以用 JPA 或 MyBatis） -->
    <dependency>
        <groupId>com.baomidou</groupId>
        <artifactId>mybatis-plus-boot-starter</artifactId>
        <version>3.5.3.1</version>
    </dependency>
    
    <!-- 连接池 -->
    <dependency>
        <groupId>com.zaxxer</groupId>
        <artifactId>HikariCP</artifactId>
    </dependency>
</dependencies>
```

### 3.2 数据库准备

```sql
-- 创建分库（假设分 2 个库，每个库 4 张表）
-- 数据库: ds_0, ds_1
-- 表: t_order_0, t_order_1, t_order_2, t_order_3

-- ds_0 数据库
CREATE DATABASE ds_0 DEFAULT CHARACTER SET utf8mb4;
USE ds_0;

CREATE TABLE t_order_0 (
    order_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    order_no VARCHAR(32) NOT NULL,
    amount DECIMAL(10,2) NOT NULL,
    status TINYINT DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id),
    INDEX idx_order_no (order_no)
) ENGINE=InnoDB;

CREATE TABLE t_order_1 LIKE t_order_0;
CREATE TABLE t_order_2 LIKE t_order_0;
CREATE TABLE t_order_3 LIKE t_order_0;

-- ds_1 数据库（结构相同）
CREATE DATABASE ds_1 DEFAULT CHARACTER SET utf8mb4;
USE ds_1;

CREATE TABLE t_order_0 (
    order_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    order_no VARCHAR(32) NOT NULL,
    amount DECIMAL(10,2) NOT NULL,
    status TINYINT DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id),
    INDEX idx_order_no (order_no)
) ENGINE=InnoDB;

CREATE TABLE t_order_1 LIKE t_order_0;
CREATE TABLE t_order_2 LIKE t_order_0;
CREATE TABLE t_order_3 LIKE t_order_0;
```

### 3.3 YAML 配置方式

```yaml
# application.yml
spring:
  shardingsphere:
    # 数据源配置
    datasource:
      names: ds_0,ds_1
      
      ds_0:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3306/ds_0?useSSL=false&serverTimezone=Asia/Shanghai&characterEncoding=utf8
        username: root
        password: root
        hikari:
          minimum-idle: 5
          maximum-pool-size: 20
          idle-timeout: 30000
          connection-timeout: 30000
          max-lifetime: 1800000
          
      ds_1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3306/ds_1?useSSL=false&serverTimezone=Asia/Shanghai&characterEncoding=utf8
        username: root
        password: root
        hikari:
          minimum-idle: 5
          maximum-pool-size: 20
    
    # 分片规则
    rules:
      sharding:
        tables:
          # 订单表分片规则
          t_order:
            # 实际数据节点
            actual-data-nodes: ds_$->{0..1}.t_order_$->{0..3}
            
            # 分库策略
            database-strategy:
              standard:
                sharding-column: user_id
                sharding-algorithm-name: database-inline
            
            # 分表策略
            table-strategy:
              standard:
                sharding-column: order_id
                sharding-algorithm-name: table-inline
            
            # 主键生成策略
            key-generate-strategy:
              column: order_id
              key-generator-name: snowflake
        
        # 分片算法配置
        sharding-algorithms:
          # 分库算法：user_id % 2
          database-inline:
            type: INLINE
            props:
              algorithm-expression: ds_$->{user_id % 2}
          
          # 分表算法：order_id % 4
          table-inline:
            type: INLINE
            props:
              algorithm-expression: t_order_$->{order_id % 4}
        
        # 主键生成器
        key-generators:
          snowflake:
            type: SNOWFLAKE
            props:
              worker-id: 1
    
    # 其他配置
    props:
      sql-show: true  # 打印 SQL（开发环境开启）
      sql-simple: false
      executor-size: 16
      max-connections-size-per-query: 1
```

### 3.4 实体类和 Mapper

```java
/**
 * 订单实体
 */
@Data
@TableName("t_order")
public class Order {
    
    @TableId(type = IdType.ASSIGN_ID)  // 使用分布式 ID
    private Long orderId;
    
    private Long userId;
    
    private String orderNo;
    
    private BigDecimal amount;
    
    private Integer status;
    
    private LocalDateTime createdAt;
}

/**
 * Mapper 接口
 */
@Mapper
public interface OrderMapper extends BaseMapper<Order> {
    
    /**
     * 根据用户 ID 查询订单（会路由到正确的分库）
     */
    @Select("SELECT * FROM t_order WHERE user_id = #{userId}")
    List<Order> selectByUserId(@Param("userId") Long userId);
    
    /**
     * 根据订单 ID 查询（会路由到正确的分表）
     */
    @Select("SELECT * FROM t_order WHERE order_id = #{orderId}")
    Order selectByOrderId(@Param("orderId") Long orderId);
    
    /**
     * 根据用户 ID 和订单 ID 查询（精确路由）
     */
    @Select("SELECT * FROM t_order WHERE user_id = #{userId} AND order_id = #{orderId}")
    Order selectByUserIdAndOrderId(@Param("userId") Long userId, @Param("orderId") Long orderId);
}
```

### 3.5 Service 层

```java
@Service
@Slf4j
public class OrderService {
    
    @Autowired
    private OrderMapper orderMapper;
    
    /**
     * 创建订单
     * ShardingSphere 会根据 user_id 和 order_id 自动路由到正确的库和表
     */
    @Transactional
    public Order createOrder(Long userId, BigDecimal amount) {
        Order order = new Order();
        // order_id 由 ShardingSphere 的雪花算法自动生成
        order.setUserId(userId);
        order.setOrderNo(generateOrderNo());
        order.setAmount(amount);
        order.setStatus(0);
        order.setCreatedAt(LocalDateTime.now());
        
        orderMapper.insert(order);
        log.info("订单创建成功: orderId={}, userId={}", order.getOrderId(), userId);
        
        return order;
    }
    
    /**
     * 根据用户 ID 查询订单
     * 只需要 user_id，会路由到正确的分库，但会扫描该库的所有分表
     */
    public List<Order> getOrdersByUserId(Long userId) {
        return orderMapper.selectByUserId(userId);
    }
    
    /**
     * 根据订单 ID 查询
     * 只有 order_id，会路由到正确的分表，但会扫描所有分库
     */
    public Order getOrderById(Long orderId) {
        return orderMapper.selectByOrderId(orderId);
    }
    
    /**
     * 精确查询（推荐）
     * 同时提供 user_id 和 order_id，可以精确路由到单个分片
     */
    public Order getOrder(Long userId, Long orderId) {
        return orderMapper.selectByUserIdAndOrderId(userId, orderId);
    }
    
    /**
     * 分页查询（跨分片）
     * 注意：跨分片分页性能较差，尽量避免深度分页
     */
    public IPage<Order> getOrdersPage(Long userId, int pageNum, int pageSize) {
        Page<Order> page = new Page<>(pageNum, pageSize);
        LambdaQueryWrapper<Order> wrapper = new LambdaQueryWrapper<>();
        wrapper.eq(Order::getUserId, userId)
               .orderByDesc(Order::getCreatedAt);
        return orderMapper.selectPage(page, wrapper);
    }
    
    private String generateOrderNo() {
        return "ORD" + System.currentTimeMillis() + RandomUtil.randomNumbers(4);
    }
}
```

### 3.6 Controller 层

```java
@RestController
@RequestMapping("/api/orders")
public class OrderController {
    
    @Autowired
    private OrderService orderService;
    
    /**
     * 创建订单
     */
    @PostMapping
    public Result<Order> createOrder(@RequestBody CreateOrderRequest request) {
        Order order = orderService.createOrder(request.getUserId(), request.getAmount());
        return Result.success(order);
    }
    
    /**
     * 查询用户订单
     */
    @GetMapping("/user/{userId}")
    public Result<List<Order>> getUserOrders(@PathVariable Long userId) {
        List<Order> orders = orderService.getOrdersByUserId(userId);
        return Result.success(orders);
    }
    
    /**
     * 查询订单详情
     */
    @GetMapping("/{orderId}")
    public Result<Order> getOrder(
            @PathVariable Long orderId,
            @RequestParam Long userId) {
        Order order = orderService.getOrder(userId, orderId);
        return Result.success(order);
    }
    
    /**
     * 分页查询
     */
    @GetMapping("/page")
    public Result<IPage<Order>> getOrdersPage(
            @RequestParam Long userId,
            @RequestParam(defaultValue = "1") int pageNum,
            @RequestParam(defaultValue = "10") int pageSize) {
        IPage<Order> page = orderService.getOrdersPage(userId, pageNum, pageSize);
        return Result.success(page);
    }
}
```

---

## 4. 分片算法详解

### 4.1 内置分片算法

ShardingSphere 提供了多种内置分片算法：

```yaml
# 1. INLINE - 行表达式分片（最常用）
sharding-algorithms:
  inline-algorithm:
    type: INLINE
    props:
      algorithm-expression: t_order_$->{order_id % 4}

# 2. MOD - 取模分片
sharding-algorithms:
  mod-algorithm:
    type: MOD
    props:
      sharding-count: 4

# 3. HASH_MOD - 哈希取模
sharding-algorithms:
  hash-mod-algorithm:
    type: HASH_MOD
    props:
      sharding-count: 4

# 4. BOUNDARY_RANGE - 边界范围分片
sharding-algorithms:
  boundary-range-algorithm:
    type: BOUNDARY_RANGE
    props:
      sharding-ranges: 1000000,2000000,3000000

# 5. VOLUME_RANGE - 容量范围分片
sharding-algorithms:
  volume-range-algorithm:
    type: VOLUME_RANGE
    props:
      range-lower: 0
      range-upper: 10000000
      sharding-volume: 1000000

# 6. AUTO_INTERVAL - 自动时间间隔分片
sharding-algorithms:
  auto-interval-algorithm:
    type: AUTO_INTERVAL
    props:
      datetime-lower: "2023-01-01 00:00:00"
      datetime-upper: "2025-01-01 00:00:00"
      sharding-seconds: 2592000  # 30天
```

### 4.2 自定义分片算法

当内置算法不满足需求时，可以自定义分片算法。

```java
/**
 * 自定义标准分片算法
 * 实现 StandardShardingAlgorithm 接口
 */
public class CustomShardingAlgorithm implements StandardShardingAlgorithm<Long> {
    
    private Properties props;
    private int shardingCount;
    
    @Override
    public void init(Properties props) {
        this.props = props;
        this.shardingCount = Integer.parseInt(props.getProperty("sharding-count", "4"));
    }
    
    /**
     * 精确分片（= 和 IN 查询）
     */
    @Override
    public String doSharding(Collection<String> availableTargetNames, 
                             PreciseShardingValue<Long> shardingValue) {
        Long value = shardingValue.getValue();
        String logicTableName = shardingValue.getLogicTableName();
        
        // 自定义分片逻辑：取模
        int index = (int) (value % shardingCount);
        String targetName = logicTableName + "_" + index;
        
        // 验证目标是否存在
        if (availableTargetNames.contains(targetName)) {
            return targetName;
        }
        
        throw new IllegalArgumentException("无法找到分片: " + targetName);
    }
    
    /**
     * 范围分片（BETWEEN 和 > < 查询）
     */
    @Override
    public Collection<String> doSharding(Collection<String> availableTargetNames,
                                         RangeShardingValue<Long> shardingValue) {
        // 范围查询需要返回所有可能的分片
        // 这里简单返回所有分片，实际可以根据范围优化
        return availableTargetNames;
    }
    
    @Override
    public Properties getProps() {
        return props;
    }
    
    @Override
    public String getType() {
        return "CUSTOM";
    }
}
```

```java
/**
 * 复合分片算法
 * 根据多个字段进行分片
 */
public class ComplexShardingAlgorithm implements ComplexKeysShardingAlgorithm<Long> {
    
    @Override
    public Collection<String> doSharding(Collection<String> availableTargetNames,
                                         ComplexKeysShardingValue<Long> shardingValue) {
        // 获取分片键值
        Map<String, Collection<Long>> columnNameAndShardingValuesMap = 
            shardingValue.getColumnNameAndShardingValuesMap();
        
        Collection<Long> userIds = columnNameAndShardingValuesMap.get("user_id");
        Collection<Long> orderIds = columnNameAndShardingValuesMap.get("order_id");
        
        Set<String> result = new HashSet<>();
        
        // 根据 user_id 和 order_id 计算分片
        for (Long userId : userIds) {
            for (Long orderId : orderIds) {
                // 自定义分片逻辑
                int dbIndex = (int) (userId % 2);
                int tableIndex = (int) (orderId % 4);
                String target = "ds_" + dbIndex + ".t_order_" + tableIndex;
                
                if (availableTargetNames.contains(target)) {
                    result.add(target);
                }
            }
        }
        
        return result;
    }
    
    @Override
    public String getType() {
        return "COMPLEX_CUSTOM";
    }
}
```

```yaml
# 配置自定义算法
spring:
  shardingsphere:
    rules:
      sharding:
        sharding-algorithms:
          custom-algorithm:
            type: CUSTOM
            props:
              sharding-count: 4
```

### 4.3 Hint 强制路由

有时候需要强制指定分片，可以使用 Hint 分片。

```java
/**
 * Hint 分片算法
 */
public class HintShardingAlgorithm implements HintShardingAlgorithm<Long> {
    
    @Override
    public Collection<String> doSharding(Collection<String> availableTargetNames,
                                         HintShardingValue<Long> shardingValue) {
        Collection<Long> values = shardingValue.getValues();
        Set<String> result = new HashSet<>();
        
        for (Long value : values) {
            for (String target : availableTargetNames) {
                if (target.endsWith("_" + (value % 4))) {
                    result.add(target);
                }
            }
        }
        
        return result;
    }
    
    @Override
    public String getType() {
        return "HINT_CUSTOM";
    }
}
```

```java
// 使用 Hint 强制路由
@Service
public class OrderService {
    
    public List<Order> getOrdersWithHint(Long tableHint) {
        // 设置 Hint
        try (HintManager hintManager = HintManager.getInstance()) {
            // 强制路由到指定表
            hintManager.addTableShardingValue("t_order", tableHint);
            
            // 或者强制路由到指定库
            hintManager.addDatabaseShardingValue("t_order", 0L);
            
            // 执行查询
            return orderMapper.selectList(null);
        }
    }
    
    /**
     * 强制走主库（读写分离场景）
     */
    public Order getOrderFromMaster(Long orderId) {
        try (HintManager hintManager = HintManager.getInstance()) {
            hintManager.setWriteRouteOnly();
            return orderMapper.selectById(orderId);
        }
    }
}
```

---

## 5. 读写分离

读写分离是提升数据库性能的常用手段，ShardingSphere 支持与分库分表结合使用。

### 5.1 读写分离配置

```yaml
spring:
  shardingsphere:
    datasource:
      names: write_ds,read_ds_0,read_ds_1
      
      write_ds:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://master:3306/demo?useSSL=false
        username: root
        password: root
        
      read_ds_0:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://slave1:3306/demo?useSSL=false
        username: root
        password: root
        
      read_ds_1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://slave2:3306/demo?useSSL=false
        username: root
        password: root
    
    rules:
      readwrite-splitting:
        data-sources:
          readwrite_ds:
            # 静态配置
            static-strategy:
              write-data-source-name: write_ds
              read-data-source-names:
                - read_ds_0
                - read_ds_1
            # 负载均衡算法
            load-balancer-name: round_robin
        
        load-balancers:
          round_robin:
            type: ROUND_ROBIN  # 轮询
          random:
            type: RANDOM       # 随机
          weight:
            type: WEIGHT       # 权重
            props:
              read_ds_0: 1
              read_ds_1: 2
```

### 5.2 分库分表 + 读写分离

```yaml
spring:
  shardingsphere:
    datasource:
      names: ds_0_master,ds_0_slave,ds_1_master,ds_1_slave
      
      ds_0_master:
        type: com.zaxxer.hikari.HikariDataSource
        jdbc-url: jdbc:mysql://master0:3306/ds_0
        username: root
        password: root
        
      ds_0_slave:
        type: com.zaxxer.hikari.HikariDataSource
        jdbc-url: jdbc:mysql://slave0:3306/ds_0
        username: root
        password: root
        
      ds_1_master:
        type: com.zaxxer.hikari.HikariDataSource
        jdbc-url: jdbc:mysql://master1:3306/ds_1
        username: root
        password: root
        
      ds_1_slave:
        type: com.zaxxer.hikari.HikariDataSource
        jdbc-url: jdbc:mysql://slave1:3306/ds_1
        username: root
        password: root
    
    rules:
      # 读写分离规则
      readwrite-splitting:
        data-sources:
          ds_0:
            static-strategy:
              write-data-source-name: ds_0_master
              read-data-source-names:
                - ds_0_slave
            load-balancer-name: round_robin
          ds_1:
            static-strategy:
              write-data-source-name: ds_1_master
              read-data-source-names:
                - ds_1_slave
            load-balancer-name: round_robin
        load-balancers:
          round_robin:
            type: ROUND_ROBIN
      
      # 分片规则（使用读写分离的逻辑数据源）
      sharding:
        tables:
          t_order:
            actual-data-nodes: ds_$->{0..1}.t_order_$->{0..3}
            database-strategy:
              standard:
                sharding-column: user_id
                sharding-algorithm-name: database-inline
            table-strategy:
              standard:
                sharding-column: order_id
                sharding-algorithm-name: table-inline
        
        sharding-algorithms:
          database-inline:
            type: INLINE
            props:
              algorithm-expression: ds_$->{user_id % 2}
          table-inline:
            type: INLINE
            props:
              algorithm-expression: t_order_$->{order_id % 4}
```

---

## 6. 分布式事务

分库分表后，跨库事务是一个挑战。ShardingSphere 支持多种分布式事务方案。

### 6.1 本地事务（默认）

```java
/**
 * 本地事务
 * 每个分片独立事务，不保证跨分片一致性
 * 适用于对一致性要求不高的场景
 */
@Service
public class OrderService {
    
    @Transactional
    public void createOrder(Order order) {
        // 如果 order 数据分布在多个分片
        // 每个分片独立提交，可能出现部分成功部分失败
        orderMapper.insert(order);
    }
}
```

### 6.2 XA 事务

```xml
<!-- 添加 XA 事务依赖 -->
<dependency>
    <groupId>org.apache.shardingsphere</groupId>
    <artifactId>shardingsphere-transaction-xa-core</artifactId>
    <version>5.3.2</version>
</dependency>
<dependency>
    <groupId>org.apache.shardingsphere</groupId>
    <artifactId>shardingsphere-transaction-xa-atomikos</artifactId>
    <version>5.3.2</version>
</dependency>
```

```yaml
# 配置 XA 事务
spring:
  shardingsphere:
    props:
      xa-transaction-manager-type: Atomikos
```

```java
/**
 * XA 分布式事务
 * 强一致性，但性能较差
 */
@Service
public class OrderService {
    
    @Transactional
    @ShardingSphereTransactionType(TransactionType.XA)
    public void createOrderWithXA(Order order, OrderItem item) {
        // 订单和订单项可能在不同分片
        // XA 事务保证要么都成功，要么都失败
        orderMapper.insert(order);
        orderItemMapper.insert(item);
    }
}
```

### 6.3 Seata AT 事务

```xml
<!-- 添加 Seata 依赖 -->
<dependency>
    <groupId>org.apache.shardingsphere</groupId>
    <artifactId>shardingsphere-transaction-base-seata-at</artifactId>
    <version>5.3.2</version>
</dependency>
<dependency>
    <groupId>io.seata</groupId>
    <artifactId>seata-spring-boot-starter</artifactId>
    <version>1.6.1</version>
</dependency>
```

```yaml
# Seata 配置
seata:
  enabled: true
  application-id: order-service
  tx-service-group: my_tx_group
  service:
    vgroup-mapping:
      my_tx_group: default
    grouplist:
      default: 127.0.0.1:8091
```

```java
/**
 * Seata AT 分布式事务
 * 最终一致性，性能较好
 */
@Service
public class OrderService {
    
    @GlobalTransactional(name = "create-order", rollbackFor = Exception.class)
    public void createOrderWithSeata(Order order, Long productId, Integer quantity) {
        // 1. 创建订单
        orderMapper.insert(order);
        
        // 2. 扣减库存（可能在不同服务/数据库）
        inventoryService.deduct(productId, quantity);
        
        // 3. 扣减余额
        accountService.deduct(order.getUserId(), order.getAmount());
        
        // Seata 保证以上操作的最终一致性
    }
}
```

### 6.4 柔性事务（推荐）

对于大多数业务场景，推荐使用柔性事务（最终一致性）。

```java
/**
 * 基于消息队列的最终一致性
 */
@Service
public class OrderService {
    
    @Autowired
    private RocketMQTemplate rocketMQTemplate;
    
    @Transactional
    public void createOrderWithMQ(Order order) {
        // 1. 本地事务：创建订单
        orderMapper.insert(order);
        
        // 2. 发送消息（库存扣减）
        // 使用事务消息保证消息一定发送成功
        rocketMQTemplate.sendMessageInTransaction(
            "order-topic",
            MessageBuilder.withPayload(order).build(),
            null
        );
    }
    
    /**
     * 消息消费者：扣减库存
     */
    @RocketMQMessageListener(topic = "order-topic", consumerGroup = "inventory-consumer")
    public class InventoryConsumer implements RocketMQListener<Order> {
        
        @Override
        public void onMessage(Order order) {
            // 扣减库存（幂等处理）
            inventoryService.deduct(order.getProductId(), order.getQuantity());
        }
    }
}
```

---

## 7. 分布式主键

分库分表后，自增 ID 会冲突，需要使用分布式主键。

### 7.1 雪花算法（Snowflake）

```
┌─────────────────────────────────────────────────────────────────────┐
│ 雪花算法 ID 结构（64位）                                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 0 - 00000000 00000000 00000000 00000000 00000000 0 - 00000 - 00000 - 000000000000
│ │   └─────────────────────────────────────────────┘   └───┘   └───┘   └──────────┘
│ │                    41位时间戳                      5位DC  5位机器   12位序列号
│ └── 1位符号位（始终为0）
│                                                                      │
│ • 时间戳: 41位，可用约69年                                           │
│ • 数据中心: 5位，最多32个数据中心                                    │
│ • 机器ID: 5位，每个数据中心最多32台机器                              │
│ • 序列号: 12位，每毫秒最多4096个ID                                   │
│                                                                      │
│ 优点: 趋势递增、不依赖数据库、高性能                                 │
│ 缺点: 依赖机器时钟、需要配置 worker-id                               │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

```yaml
# ShardingSphere 雪花算法配置
spring:
  shardingsphere:
    rules:
      sharding:
        key-generators:
          snowflake:
            type: SNOWFLAKE
            props:
              worker-id: 1  # 机器ID，需要保证唯一
```

### 7.2 UUID

```java
/**
 * UUID 主键
 * 优点: 简单，不依赖任何服务
 * 缺点: 无序，索引性能差，占用空间大
 */
@Data
@TableName("t_order")
public class Order {
    
    @TableId(type = IdType.ASSIGN_UUID)
    private String orderId;  // 使用 String 类型
    
    // ...
}
```

### 7.3 号段模式（推荐）

```java
/**
 * 号段模式
 * 从数据库批量获取 ID 段，本地分配
 * 优点: 高性能，趋势递增
 * 缺点: 需要额外的号段表
 */

// 号段表
// CREATE TABLE id_generator (
//     biz_type VARCHAR(32) PRIMARY KEY,
//     max_id BIGINT NOT NULL,
//     step INT NOT NULL DEFAULT 1000,
//     version INT NOT NULL DEFAULT 0,
//     updated_at DATETIME
// );

@Service
public class SegmentIdGenerator {
    
    @Autowired
    private JdbcTemplate jdbcTemplate;
    
    private final Map<String, Segment> segmentMap = new ConcurrentHashMap<>();
    
    /**
     * 获取下一个 ID
     */
    public long nextId(String bizType) {
        Segment segment = segmentMap.computeIfAbsent(bizType, k -> new Segment());
        
        synchronized (segment) {
            if (segment.currentId >= segment.maxId) {
                // 当前号段用完，获取新号段
                loadNewSegment(bizType, segment);
            }
            return segment.currentId++;
        }
    }
    
    /**
     * 从数据库加载新号段
     */
    private void loadNewSegment(String bizType, Segment segment) {
        // 乐观锁更新
        String sql = "UPDATE id_generator SET max_id = max_id + step, version = version + 1, " +
                     "updated_at = NOW() WHERE biz_type = ? AND version = ?";
        
        int rows = 0;
        int retries = 3;
        
        while (rows == 0 && retries-- > 0) {
            // 查询当前值
            Map<String, Object> result = jdbcTemplate.queryForMap(
                "SELECT max_id, step, version FROM id_generator WHERE biz_type = ?", bizType);
            
            long maxId = (Long) result.get("max_id");
            int step = (Integer) result.get("step");
            int version = (Integer) result.get("version");
            
            // 尝试更新
            rows = jdbcTemplate.update(sql, bizType, version);
            
            if (rows > 0) {
                segment.currentId = maxId;
                segment.maxId = maxId + step;
            }
        }
        
        if (rows == 0) {
            throw new RuntimeException("获取号段失败");
        }
    }
    
    private static class Segment {
        long currentId;
        long maxId;
    }
}
```

### 7.4 美团 Leaf

```xml
<!-- 使用美团 Leaf（推荐生产使用） -->
<dependency>
    <groupId>com.sankuai.inf.leaf</groupId>
    <artifactId>leaf-boot-starter</artifactId>
    <version>1.0.1</version>
</dependency>
```

```yaml
# Leaf 配置
leaf:
  name: order-service
  segment:
    enable: true
    jdbc-url: jdbc:mysql://localhost:3306/leaf
    jdbc-username: root
    jdbc-password: root
  snowflake:
    enable: true
    zk-address: localhost:2181
```

```java
@Service
public class OrderService {
    
    @Autowired
    private SegmentService segmentService;
    
    @Autowired
    private SnowflakeService snowflakeService;
    
    public Long generateOrderId() {
        // 号段模式
        Result result = segmentService.getId("order");
        return result.getId();
        
        // 或雪花算法
        // Result result = snowflakeService.getId("order");
        // return result.getId();
    }
}
```

---

## 8. 数据迁移

### 8.1 迁移方案选择

```
┌─────────────────────────────────────────────────────────────────────┐
│ 数据迁移方案对比                                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 方案              │ 停机时间 │ 复杂度 │ 数据一致性 │ 适用场景       │
│───────────────────┼──────────┼────────┼────────────┼────────────────│
│ 停机迁移          │ 长       │ 低     │ 高         │ 数据量小       │
│ 双写迁移          │ 短       │ 中     │ 中         │ 中等数据量     │
│ 增量同步          │ 极短     │ 高     │ 高         │ 大数据量       │
│ 影子表            │ 无       │ 高     │ 高         │ 核心业务       │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 8.2 双写迁移方案

```java
/**
 * 双写迁移方案
 * 1. 新老库同时写入
 * 2. 历史数据迁移
 * 3. 数据校验
 * 4. 切换读取
 * 5. 停止老库写入
 */
@Service
public class OrderMigrationService {
    
    @Autowired
    private OldOrderMapper oldOrderMapper;  // 老库
    
    @Autowired
    private NewOrderMapper newOrderMapper;  // 新库（分库分表）
    
    @Autowired
    private RedisTemplate<String, String> redisTemplate;
    
    /**
     * 双写：同时写入新老库
     */
    @Transactional
    public void createOrderWithDoubleWrite(Order order) {
        // 1. 写入老库
        oldOrderMapper.insert(order);
        
        // 2. 异步写入新库（或同步，取决于一致性要求）
        CompletableFuture.runAsync(() -> {
            try {
                newOrderMapper.insert(order);
            } catch (Exception e) {
                // 记录失败，后续补偿
                log.error("写入新库失败: {}", order.getOrderId(), e);
                recordFailedOrder(order.getOrderId());
            }
        });
    }
    
    /**
     * 历史数据迁移
     */
    public void migrateHistoryData() {
        Long lastId = getLastMigratedId();
        int batchSize = 1000;
        
        while (true) {
            // 分批查询老库数据
            List<Order> orders = oldOrderMapper.selectBatch(lastId, batchSize);
            if (orders.isEmpty()) {
                break;
            }
            
            // 批量写入新库
            for (Order order : orders) {
                try {
                    newOrderMapper.insert(order);
                } catch (DuplicateKeyException e) {
                    // 已存在，跳过
                }
            }
            
            // 记录进度
            lastId = orders.get(orders.size() - 1).getOrderId();
            saveLastMigratedId(lastId);
            
            log.info("已迁移到 ID: {}", lastId);
        }
    }
    
    /**
     * 数据校验
     */
    public void verifyData() {
        Long lastId = 0L;
        int batchSize = 1000;
        int mismatchCount = 0;
        
        while (true) {
            List<Order> oldOrders = oldOrderMapper.selectBatch(lastId, batchSize);
            if (oldOrders.isEmpty()) {
                break;
            }
            
            for (Order oldOrder : oldOrders) {
                Order newOrder = newOrderMapper.selectById(oldOrder.getOrderId());
                
                if (newOrder == null) {
                    log.error("新库缺失数据: {}", oldOrder.getOrderId());
                    mismatchCount++;
                } else if (!oldOrder.equals(newOrder)) {
                    log.error("数据不一致: {}", oldOrder.getOrderId());
                    mismatchCount++;
                }
            }
            
            lastId = oldOrders.get(oldOrders.size() - 1).getOrderId();
        }
        
        log.info("校验完成，不一致数量: {}", mismatchCount);
    }
    
    private void recordFailedOrder(Long orderId) {
        redisTemplate.opsForSet().add("migration:failed", orderId.toString());
    }
    
    private Long getLastMigratedId() {
        String id = redisTemplate.opsForValue().get("migration:lastId");
        return id != null ? Long.parseLong(id) : 0L;
    }
    
    private void saveLastMigratedId(Long id) {
        redisTemplate.opsForValue().set("migration:lastId", id.toString());
    }
}
```

### 8.3 使用 Canal 增量同步

```yaml
# Canal 配置
canal:
  server:
    host: localhost
    port: 11111
  destination: example
  username: canal
  password: canal
```

```java
/**
 * Canal 消费者
 * 监听老库 binlog，同步到新库
 */
@Component
public class CanalConsumer {
    
    @Autowired
    private NewOrderMapper newOrderMapper;
    
    @CanalEventListener
    public void onEvent(CanalEntry.Entry entry) {
        if (entry.getEntryType() != CanalEntry.EntryType.ROWDATA) {
            return;
        }
        
        CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(entry.getStoreValue());
        CanalEntry.EventType eventType = rowChange.getEventType();
        
        for (CanalEntry.RowData rowData : rowChange.getRowDatasList()) {
            if (eventType == CanalEntry.EventType.INSERT) {
                handleInsert(rowData);
            } else if (eventType == CanalEntry.EventType.UPDATE) {
                handleUpdate(rowData);
            } else if (eventType == CanalEntry.EventType.DELETE) {
                handleDelete(rowData);
            }
        }
    }
    
    private void handleInsert(CanalEntry.RowData rowData) {
        Order order = parseOrder(rowData.getAfterColumnsList());
        newOrderMapper.insert(order);
    }
    
    private void handleUpdate(CanalEntry.RowData rowData) {
        Order order = parseOrder(rowData.getAfterColumnsList());
        newOrderMapper.updateById(order);
    }
    
    private void handleDelete(CanalEntry.RowData rowData) {
        Long orderId = parseOrderId(rowData.getBeforeColumnsList());
        newOrderMapper.deleteById(orderId);
    }
    
    private Order parseOrder(List<CanalEntry.Column> columns) {
        Order order = new Order();
        for (CanalEntry.Column column : columns) {
            switch (column.getName()) {
                case "order_id":
                    order.setOrderId(Long.parseLong(column.getValue()));
                    break;
                case "user_id":
                    order.setUserId(Long.parseLong(column.getValue()));
                    break;
                // ... 其他字段
            }
        }
        return order;
    }
}
```

---

## 9. 运维与监控

### 9.1 SQL 审计

```yaml
# 开启 SQL 日志
spring:
  shardingsphere:
    props:
      sql-show: true
      sql-simple: false
```

```java
/**
 * 自定义 SQL 审计
 */
@Component
public class SqlAuditListener implements SQLExecutionHook {
    
    @Override
    public void start(String dataSourceName, String sql, List<Object> parameters,
                      DataSourceMetaData dataSourceMetaData, boolean isTrunkThread,
                      Map<String, Object> shardingExecuteDataMap) {
        // SQL 执行前
        log.info("执行 SQL: datasource={}, sql={}, params={}", 
                dataSourceName, sql, parameters);
    }
    
    @Override
    public void finishSuccess() {
        // SQL 执行成功
    }
    
    @Override
    public void finishFailure(Exception cause) {
        // SQL 执行失败
        log.error("SQL 执行失败", cause);
    }
}
```

### 9.2 监控指标

```java
/**
 * 分库分表监控指标
 */
@Component
public class ShardingMetrics {
    
    private final MeterRegistry meterRegistry;
    
    // SQL 执行计数
    private final Counter sqlCounter;
    
    // SQL 执行耗时
    private final Timer sqlTimer;
    
    // 跨分片查询计数
    private final Counter crossShardCounter;
    
    public ShardingMetrics(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        
        this.sqlCounter = Counter.builder("sharding.sql.count")
                .description("SQL 执行次数")
                .register(meterRegistry);
        
        this.sqlTimer = Timer.builder("sharding.sql.duration")
                .description("SQL 执行耗时")
                .register(meterRegistry);
        
        this.crossShardCounter = Counter.builder("sharding.cross.shard.count")
                .description("跨分片查询次数")
                .register(meterRegistry);
    }
    
    public void recordSql(String sql, long duration, int shardCount) {
        sqlCounter.increment();
        sqlTimer.record(duration, TimeUnit.MILLISECONDS);
        
        if (shardCount > 1) {
            crossShardCounter.increment();
        }
    }
}
```

### 9.3 健康检查

```java
/**
 * 分库分表健康检查
 */
@Component
public class ShardingHealthIndicator implements HealthIndicator {
    
    @Autowired
    private DataSource dataSource;
    
    @Override
    public Health health() {
        try {
            // 检查所有分片连接
            if (dataSource instanceof ShardingSphereDataSource) {
                ShardingSphereDataSource shardingDataSource = (ShardingSphereDataSource) dataSource;
                Map<String, DataSource> dataSourceMap = shardingDataSource.getDataSourceMap();
                
                Map<String, String> details = new HashMap<>();
                for (Map.Entry<String, DataSource> entry : dataSourceMap.entrySet()) {
                    try (Connection conn = entry.getValue().getConnection()) {
                        details.put(entry.getKey(), "UP");
                    } catch (Exception e) {
                        details.put(entry.getKey(), "DOWN: " + e.getMessage());
                        return Health.down()
                                .withDetails(details)
                                .build();
                    }
                }
                
                return Health.up()
                        .withDetails(details)
                        .build();
            }
            
            return Health.up().build();
        } catch (Exception e) {
            return Health.down(e).build();
        }
    }
}
```

---

## 10. 常见错误与解决方案

### 10.1 分片键相关问题

```java
// ❌ 错误 1：查询不带分片键
// 会导致全库全表扫描
@Select("SELECT * FROM t_order WHERE order_no = #{orderNo}")
Order selectByOrderNo(String orderNo);

// ✅ 解决：查询条件包含分片键
@Select("SELECT * FROM t_order WHERE user_id = #{userId} AND order_no = #{orderNo}")
Order selectByUserIdAndOrderNo(Long userId, String orderNo);

// 或者建立 order_no 到 user_id 的映射关系
// 先查映射，再查订单

// ❌ 错误 2：更新分片键
// 分片键不能更新，会导致数据路由错误
@Update("UPDATE t_order SET user_id = #{newUserId} WHERE order_id = #{orderId}")
void updateUserId(Long orderId, Long newUserId);

// ✅ 解决：分片键设计时就要考虑不可变性
// 如果必须更新，需要删除旧数据，插入新数据

// ❌ 错误 3：分片键为 NULL
// NULL 值无法正确路由
Order order = new Order();
order.setUserId(null);  // ❌
orderMapper.insert(order);

// ✅ 解决：分片键必须有值
if (order.getUserId() == null) {
    throw new IllegalArgumentException("用户ID不能为空");
}
```

### 10.2 跨分片查询问题

```java
// ❌ 错误 1：跨分片 JOIN
// ShardingSphere 不支持跨库 JOIN
@Select("SELECT o.*, u.username FROM t_order o " +
        "JOIN t_user u ON o.user_id = u.user_id")
List<OrderVO> selectWithUser();

// ✅ 解决方案 1：应用层组装
public List<OrderVO> getOrdersWithUser(Long userId) {
    List<Order> orders = orderMapper.selectByUserId(userId);
    User user = userMapper.selectById(userId);
    
    return orders.stream().map(order -> {
        OrderVO vo = new OrderVO();
        BeanUtils.copyProperties(order, vo);
        vo.setUsername(user.getUsername());
        return vo;
    }).collect(Collectors.toList());
}

// ✅ 解决方案 2：数据冗余
// 在订单表中冗余用户名
@TableName("t_order")
public class Order {
    private Long orderId;
    private Long userId;
    private String username;  // 冗余字段
    // ...
}

// ✅ 解决方案 3：广播表
// 将小表配置为广播表，每个分片都有完整数据
spring:
  shardingsphere:
    rules:
      sharding:
        broadcast-tables:
          - t_config
          - t_dict
```

### 10.3 分页排序问题

```java
// ❌ 错误：深度分页
// 跨分片分页性能很差
@Select("SELECT * FROM t_order ORDER BY created_at DESC LIMIT 100000, 10")
List<Order> selectPage();
// 需要从每个分片查询 100010 条，然后归并排序

// ✅ 解决方案 1：限制分页深度
public IPage<Order> getOrders(int pageNum, int pageSize) {
    if (pageNum > 100) {
        throw new BusinessException("页码不能超过100");
    }
    // ...
}

// ✅ 解决方案 2：使用游标分页
@Select("SELECT * FROM t_order WHERE user_id = #{userId} AND order_id < #{lastOrderId} " +
        "ORDER BY order_id DESC LIMIT #{limit}")
List<Order> selectByLastId(Long userId, Long lastOrderId, int limit);

// ✅ 解决方案 3：带分片键分页
// 如果查询条件包含分片键，只会查询单个分片
@Select("SELECT * FROM t_order WHERE user_id = #{userId} ORDER BY created_at DESC")
IPage<Order> selectPageByUserId(IPage<Order> page, Long userId);
```

### 10.4 事务问题

```java
// ❌ 错误：跨分片事务不一致
@Transactional
public void createOrderAndDeductStock(Order order, Long productId) {
    // 订单和库存可能在不同分片
    // 默认本地事务，可能出现部分成功
    orderMapper.insert(order);
    stockMapper.deduct(productId, 1);
}

// ✅ 解决方案 1：使用 XA 事务（强一致，性能差）
@Transactional
@ShardingSphereTransactionType(TransactionType.XA)
public void createOrderAndDeductStock(Order order, Long productId) {
    orderMapper.insert(order);
    stockMapper.deduct(productId, 1);
}

// ✅ 解决方案 2：使用 Seata（最终一致，性能好）
@GlobalTransactional
public void createOrderAndDeductStock(Order order, Long productId) {
    orderMapper.insert(order);
    stockMapper.deduct(productId, 1);
}

// ✅ 解决方案 3：业务设计避免跨分片事务
// 将相关数据放在同一分片
// 例如：订单和订单项使用相同的分片键
```

### 10.5 主键冲突

```java
// ❌ 错误：使用数据库自增 ID
@TableId(type = IdType.AUTO)
private Long orderId;
// 不同分片的自增 ID 会冲突

// ✅ 解决：使用分布式 ID
@TableId(type = IdType.ASSIGN_ID)  // MyBatis-Plus 雪花算法
private Long orderId;

// 或配置 ShardingSphere 主键生成
spring:
  shardingsphere:
    rules:
      sharding:
        tables:
          t_order:
            key-generate-strategy:
              column: order_id
              key-generator-name: snowflake
```

### 10.6 配置问题

```yaml
# ❌ 错误：actual-data-nodes 配置错误
actual-data-nodes: ds_$->{0..1}.t_order_$->{0..3}
# 实际数据库/表名必须与配置一致

# ❌ 错误：分片算法表达式错误
algorithm-expression: ds_$->{user_id % 2}
# 如果 user_id 可能为 null，会报错

# ✅ 解决：添加空值处理
algorithm-expression: ds_$->{user_id == null ? 0 : user_id % 2}

# ❌ 错误：数据源名称不匹配
datasource:
  names: ds0,ds1  # 名称
rules:
  sharding:
    tables:
      t_order:
        actual-data-nodes: ds_$->{0..1}.t_order  # 使用了 ds_0, ds_1

# ✅ 解决：保持名称一致
datasource:
  names: ds_0,ds_1
```

### 10.7 常见错误速查表

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分库分表常见错误速查表                                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 错误现象                    │ 可能原因                │ 解决方案    │
│─────────────────────────────┼────────────────────────┼─────────────│
│ 全库扫描                    │ 查询不带分片键         │ 添加分片键  │
│ 数据路由错误                │ 分片键值为 NULL        │ 非空校验    │
│ 主键冲突                    │ 使用自增 ID            │ 分布式 ID   │
│ 跨库 JOIN 失败              │ 不支持跨库 JOIN        │ 应用层组装  │
│ 分页数据不对                │ 跨分片分页归并问题     │ 带分片键查询│
│ 事务不一致                  │ 跨分片本地事务         │ 分布式事务  │
│ 配置不生效                  │ 数据源名称不匹配       │ 检查配置    │
│ 性能差                      │ 深度分页/全表扫描      │ 优化查询    │
│ 连接池耗尽                  │ 分片数×连接数过多      │ 调整连接池  │
│ 数据倾斜                    │ 分片键选择不当         │ 重新设计    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 附录

### A. 分库分表检查清单

```
□ 是否真的需要分库分表？（先考虑读写分离、缓存、索引优化）
□ 分片键选择是否合理？（查询频率、数据分布、不可变性）
□ 分片数量是否预留了扩展空间？
□ 是否处理了分布式主键？
□ 跨分片查询是否有优化方案？
□ 分布式事务方案是否确定？
□ 数据迁移方案是否可行？
□ 监控和告警是否完善？
□ 回滚方案是否准备好？
```

### B. 性能优化建议

```
┌─────────────────────────────────────────────────────────────────────┐
│ 分库分表性能优化                                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ 1. 查询优化                                                          │
│    • 查询条件必须包含分片键                                          │
│    • 避免跨分片 JOIN，使用应用层组装                                 │
│    • 限制分页深度，使用游标分页                                      │
│    • 使用覆盖索引减少回表                                            │
│                                                                      │
│ 2. 写入优化                                                          │
│    • 批量插入代替单条插入                                            │
│    • 异步写入非核心数据                                              │
│    • 使用本地事务代替分布式事务                                      │
│                                                                      │
│ 3. 连接池优化                                                        │
│    • 合理设置连接池大小                                              │
│    • 总连接数 = 分片数 × 单分片连接数                                │
│    • 避免连接泄漏                                                    │
│                                                                      │
│ 4. 缓存优化                                                          │
│    • 热点数据使用 Redis 缓存                                         │
│    • 使用本地缓存减少网络开销                                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### C. 相关资源

```
• ShardingSphere 官方文档: https://shardingsphere.apache.org/document/
• ShardingSphere GitHub: https://github.com/apache/shardingsphere
• 美团 Leaf: https://github.com/Meituan-Dianping/Leaf
• Seata: https://seata.io/
• Canal: https://github.com/alibaba/canal
```

### D. 版本兼容性

```
┌─────────────────────────────────────────────────────────────────────┐
│ 版本兼容性参考                                                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│ ShardingSphere 5.x:                                                 │
│ • Java 8+                                                           │
│ • Spring Boot 2.x / 3.x                                             │
│ • MySQL 5.7+ / 8.0+                                                 │
│ • MyBatis 3.5+ / MyBatis-Plus 3.4+                                  │
│                                                                      │
│ 本笔记基于:                                                          │
│ • Java 8                                                            │
│ • Spring Boot 2.7.18                                                │
│ • ShardingSphere 5.3.2                                              │
│ • MyBatis-Plus 3.5.3.1                                              │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

> 最后更新: 2025年1月
> 
> 分库分表是一把双刃剑，带来扩展性的同时也带来复杂性。在决定分库分表之前，请先尝试其他优化手段（索引优化、读写分离、缓存）。如果确实需要分库分表，请做好充分的规划和测试！
